# Relative
if (! (Relative %in% c("TRUE", "FALSE"))) {
return ("Relative must be TRUE or FALSE !")
}
# Verbose
if (! (Verbose %in% c("TRUE", "FALSE"))) {
return ("Verbose must be TRUE or FALSE !")
}
# Number of perturbations
if (nbP < nbN)
return ("Not enough perturbations to get result !")
if (! is.null(MapExper) && (nbP < (nbN-1+nbM)))
return ("Not enough perturbations to get result !")
return (NULL)			# No error detected
}		# CheckInputData
#' Computes the "Connectivity Matrix" using ELASTICNET, RIDGE or LASSO Method (library glmnet)
#'
#'@param MatU		Used to compute linear Regression
#'@param MatY		Y as a function of U
#'@param Lbda		Hyper parameter λ of the method  -- If Lbda = NULL, a "best value" of Lbda is computed directly
#'@param Mu			Hyper parameter μ of the method  -- Mu = 0 : RIDGE, Mu = 1 : LASSO, else (between 0 and 1) : Elastic Net.
#'@param nbN		Number of nodes
#'
#'
#' @return 			NULL if an error is detected or the result ("r[iNode, ]") otherwise (vector, length = nbN)
#'
fELASTIC	<- function (MatU, MatY, Lbda, Mu, nbN) {
tryCatch (
expr = {
rL 			<- vector(length=nbN)													# Result of Lasso method (ie sol. of Yi = Ai * Ui)
if (is.null(Lbda)) {
cv_model 	<- cv.glmnet(MatU, MatY, alpha = Mu, grouped = FALSE)				# Fits lasso regression model using k-fold cross-validation
Lbda		<- cv_model$lambda.min
}
best_model 	<- glmnet(MatU, MatY, alpha = Mu, lambda = Lbda, grouped = FALSE) 		# View coefficients of best model
rL			<-	coef(best_model)
return (rL)
},		# expr
error = function (e) {
message ("Error detected in ELASTIC method !")
print(e)
}		# error
)			# tryCatch
return (NULL)
}		# fELASTIC
#' Computes the "Connectivity Matrix" using STEP Method
#'
#'@param MatU		Used to compute linear Regression
#'@param MatY		Y as a function of U
#'@param Meth		Hyper parameter of the method  -- "Fo" => "STEP-Fo", "Ba" => "STEP-Ba", "Bo" => "STEP-Bo"
#'@param nbN		Number of nodes
#'@param Verbose	Logical. If TRUE, additional printings are made. These printings are for internal use only, so they are not documented.
#'
#'
#' @return 			NULL if an error is detected or the result ("r[iNode, ]") otherwise  (vector, length = nbN-1)
#'
fSTEP	<- function (MatU, MatY, Meth, nbN, Verbose) {
tryCatch (
expr = {
rij 		<- matrix(nrow=1, ncol=nbN-1)			# Intermediate calculation of rij - Use a matrix instead of a vector to use 'colnames'
Donnees  <- data.frame(MatY, MatU)
if (Verbose) {
cat(" Data ", colnames(Donnees), "\n")
}
Donnees	 <- rename(Donnees, "Y" ="MatY")			# The follow-up is clearer like this
cc 		 <- colnames(Donnees)						# Name of the columns "Data". The first one is "Y"
cc		 <- cc[-1]									# It remains the name of the coefficients ("X1", "X2", ... "Xn"  -- n = nbN-1)
colnames(rij) <- cc									# STEP gives the column names corresponding to the coefficients that are kept
switch(Meth,
"Fo" =
{intercept_only <- lm(Y ~ 1, data=Donnees)
all 	<- lm(Y ~ ., data=Donnees)
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0)},	# Forward
"Ba" =
{intercept_only <- lm(Y ~ 1, data=Donnees)
all 	<- lm(Y ~ ., data=Donnees)
forward <- step(all, direction='backward', scope=formula(all), trace=0)},				# Backward
"Bo" =
{intercept_only <- lm(Y ~ 1, data=Donnees)
all 	<- lm(Y ~ ., data=Donnees)
forward <- step(intercept_only, direction='both', scope=formula(all), trace=0)},		# Both
)
ll  	<- length(forward$coefficients)				# Number of coefficients kept by step
nn		<- names(forward$coefficients)				# Name of these coefficients
rij[1, ]	<- 0
if (ll >= 2) {
for (i in 2:ll) {								# nn[1] = "(Intercept)"
rij[1, nn[i]] <- forward$coefficients[nn[i]]
}
}
if (Verbose) {
cat (" ll ", ll, " nn ", nn, " rij ", rij, "\n")
}
return(rij[1, ])
},		# expr
error = function (e) {
message ("Error detected in STEP method !")
print(e)
}		# error
)			# tryCatch
return (NULL)
}		# fSTEP
#' Computes the "Connectivity Matrix" using Random Forest Method (library randomForest)
#'
#'@param MatU		Used to compute linear Regression
#'@param MatY		Y as a function of U
#'@param seqLetters	A sequence of lower case letters "a", "b" .... used to name the columns
#'@param nbN		Number of nodes
#'
#'
#' @return 			NULL if an error is detected or the result ("r[iNode, ]") otherwise  (vector, length = nbN-1)
#'
fRForest	<- function (MatU, MatY, seqLetters, nbN) {
tryCatch (
expr = {
rL 			<- vector(length=nbN-1)					# Result of Random Forest method
nbLmin		<- 80									# minimum number of rows to use Random Forest method
MatA		<- cbind(MatY, MatU)
nr			<- nrow(MatA)
nrep		<- round(nbLmin/nr)
MatB		<- MatA
for (i in 1:nrep) {									# to have enough rows
MatB	<- rbind(MatB, MatA)
}
colnames(MatB)	<- c("Y", seqLetters)
rf			<- randomForest(Y ~ ., data=MatB, importance=TRUE)
rL			<- importance(rf)[,1]/sum(importance(rf)[,1])
return (rL)
},		# expr
error = function (e) {
message ("Error detected in Random Forest method !")
print(e)
}		# error
)			# tryCatch
return (NULL)
}		# fRForest
#' Computes a sequence of letters : "a", "b", "c", ...., "z", "aa", "ab", .... , "az", "ba", "bb" etc ....
#'
#'@param	n		The number of elements of the sequence
#'
#'
#' @return			A vector, length n, containing the sequence of letters
#'
sequence_letters <- function(n) {
tryCatch (
expr = {
seq <- character(n)
letters <- c(letters, "")
for (i in 1:n) {
indices <- c()
k <- i - 1
while (k >= 0) {
indices <- c((k %% 26) + 1, indices)
k <- k %/% 26 - 1
}
seq[i] <- paste0(letters[indices], collapse = "")
}
return(seq)
},		# expr
error = function (e) {
message ("Error detected in sequence_letters function!")
print(e)
}		# error
)			# tryCatch
return (NULL)
}		# sequence_letters
#' Computes a squared value
#'
#'@param	x		The value to square (may be a vector)
#'
#'
#' @return			A vector, with the values squared
#'
fcarre	= function(x) {x^2}
Res2	<- MRARegress(MExp2, Pert2, Relative=FALSE)
Res2$ANOVA
library(MRARegress)
nbN	<-  3			# Nombre de noeuds
F3Nds <- function(P,X)
c(F1 = (25*(20*P[1]-X[1])/((20+20*P[1]-X[1])*(5+X[3]))-10*X[1]/(20+X[1])),
F2 = (3*(20*P[2]-X[2])*X[1]/(20+20*P[2]-X[2])-10*X[2]/(20+X[2])),
F3 = ((20*P[3]-X[3])*X[2]/(20+20*P[3]-X[3])-10*X[3]/(20+X[3])))
# où X représente les niveaux d'expression de pRaF, ppMEK, ppERK respectivement.
st 	<- c(0.1, 0.1, 0.1)	# Point de départ de la recherche d'optimum. De ce point, on converge vers la solution.
P0 	<- c(1, 1, 1)		 # Valeur initiale des paramètres
Perturbs  <- c(0.2, 0.9, 0.99)	 # Trois perturbations : -80%, -10%, -1%
MExp_	<- MExp(nbN, Perturbs, P0, F3Nds, st)
MExp_
set.seed(12345)
Bruit	<- matrix(rnorm(30, 0, 0.01), nrow=3)
MExp1	<- MExp_$Exp + Bruit
MExp2	<- cbind(MExp_$Exp, MExp1[ ,2:10])
Pert2	<- c(MExp_$Pert, MExp_$Pert[2:10])
Res2	<- MRARegress(MExp2, Pert2, Relative=FALSE)
Res2$ANOVA
Res_O2 <- MRARegress(MExp_$Exp,MExp_$Pert, Method="Order2", Relative=FALSE)
Res_O2$r
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
vignette
?vignette
vignette("MRARegress.html", package = "MRARegress")
vignette("MRARegress", package = "MRARegress", lib.loc = "C:/Users/jean-pierre.borg/IRCM/These/Recherche/Packages/MRARegress/vignettes")
vignette("MRARegress", package = "MRARegress")
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
devtools::install_github("J-P-Borg/MRARegress", build = TRUE, build_opts = c("--no-resave-data", "--no-manual"))
detach("package:MRARegress", unload = TRUE)
devtools::install_github("J-P-Borg/MRARegress", build = TRUE, build_opts = c("--no-resave-data", "--no-manual"))
library(MRARegress)
?MRARegress
?MRARegress
?stats
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
devtools::use_vignette
build_vignette()
?devtools
library(MRARegress)
vignette("MRARegress", package = "MRARegress")
devtools::install_github(build_vignettes = TRUE)
devtools::install_github("J-P-Borg/MRARegress", build_vignettes = TRUE)
vignette("MRARegress", package = "MRARegress")
detach("package:MRARegress", unload = TRUE)
devtools::install_github("J-P-Borg/MRARegress", build_vignettes = TRUE)
vignette("MRARegress", package = "MRARegress")
vignette("MRARegress")
devtools::build(vignettes=TRUE)
vignette("MRARegress", package = "MRARegress")
devtools::use_vignette("MRARegress")
browseVignettes("MRARegress")
devtools::build()
devtools::install(build_vignettes=TRUE)
library(htmltools)
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
library(htmltools)
remove.packages("htmltools")
install.packages("htmltools")
?MRARegress
library(MRARegress)
?MRARegress
library(MRARegress)
?MRARegress
?stats
?MRARegress
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
detach("package:MRARegress", unload = TRUE)
devtools::build()
devtools::install(build_vignettes=TRUE)
vignette("MRARegress", package = "MRARegress")
?stats
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
library(MRARegress)
?MRARegress
vignette("MRARegress", package = "MRARegress")
MExp1	<- c(1,2,3, 11,21,31, 12,22,32, 13,23,33)
Res1 = MRARegress(MExp1, Relative=FALSE)
MExp1
MExp1	<- matrix(c(1,2,3, 11,21,31, 12,22,32, 13,23,33), nrow=3)
MExp1
Res1 = MRARegress(MExp1, Relative=FALSE)
Res1$r
MExp2	<- matrix(c(1,2,3, 11,21,31, 12,22,32, 13,23,33, 14,24,34, 15,25,35, 16,26,36), nrow=3)
MExp2
MExp2	<- matrix(c(1,2,3, 11,21,31, 12,22,32, 13,23,33, 14,24,34, 15,25,35, 16,26,36), nrow=3)
Pert2 	<- c("Base", "Q1 -> N1", "Q2 -> N1", "Q3 -> N1", "Q4 -> N2", "Q5 -> N2", "Q6 -> N3")
Res2 	<- MRARegress(MExp2, Pert2, Relative=FALSE)
Res2$r
X1	<- (pi/4)*cos(pi/4)		# 0.5553604
X2	<- (pi/4)*sin(pi/4)		# 0.5553604
# Q1 : on perturbe  ρ  : 0.99 vs 1
X11	<- 0.99*X1			# 0.5498068
X21	<- 0.99*X2			# 0.549806
# Q3 : on perturbe θ  :  0.80*pi/4 vs pi/4
X13	<- (0.8*pi/4)*cos(0.8*pi/4)
X23	<- (0.8*pi/4)*sin(0.8*pi/4)
# Q4 : on perturbe θ  : 0.70*pi/4 vs pi/4
X14	<- (0.7*pi/4)*cos(0.7*pi/4)
X24	<- (0.7*pi/4)*sin(0.7*pi/4)
MExp3	<- matrix(c(X1,X2, X11,X21, X13,X23, X14,X24), nrow=2)
MExp3
MExp3	<- matrix(c(X1,X2, X11,X21, X13,X23, X14,X24), nrow=2)
Pert3	<- c("Base", "Q1->N1", "Q3->N2", "Q4->N2")
# Je triche
Res 	<- MRARegress (MExp3, Pert3, Relative=FALSE)$r
Res
MOper3	<- matrix(c(1,0, 0,2, 0,3), nrow=2)
PNode3	<- matrix(c(1,1, 1,1), nrow=2)
Res 	<- MRARegress (MExp3, Pert3, MapExper=MOper3, ParNode=PNode3, Relative=FALSE)$r
Res
MOper3	<- matrix(c(1,0, 0,2, 0,3), nrow=2)
PNode3	<- matrix(c(1,1, 1,1), nrow=2)
Res 	<- MRARegress (MExp3, Pert3, MapExper=MOper3, ParNode=PNode3, Relative=FALSE)
Res
?MRARegress
load ("C:\\Users\\jean-pierre.borg\\IRCM\\These\\Recherche\\Packages\\MRARegress\\data-raw\\Jimenez_Fig3.rda")		# data
Nodes 	<- c('LCOR', 'RIP140', 'Hoxa5', 'Luciferase')
Pert 	<- c('Base', 'E2+RA+siLCoR->LCOR', 'E2+RA+siRIP140->RIP140', 'E2->Hoxa5', 'RA->Luciferase')
MatExp_Jim	<- data$mean[c('LCoR','RIP140','Hoxa5','Luciferase'), c('E2+RA', 'E2+RA+siLCoR', 'E2+RA+siRIP140', 'E2', 'RA')]
Res		<- MRARegress(MatExp_Jim, Pert, Nodes)
Res1	<- round(Res$r, 3)
Res1
Drawgraph(Res1)
DrawGraph(Res1)
nbN	<-  3			# Nombre de noeuds
F3Nds 	<- function(P,X)
c(F1 = (25*(20*P[1]-X[1])/((20+20*P[1]-X[1])*(5+X[3]))-10*X[1]/(20+X[1])),
F2 = (3*(20*P[2]-X[2])*X[1]/(20+20*P[2]-X[2])-10*X[2]/(20+X[2])),
F3 = ((20*P[3]-X[3])*X[2]/(20+20*P[3]-X[3])-10*X[3]/(20+X[3])))
# où X représente les niveaux d'expression de pRaF, ppMEK, ppERK respectivement.
st 		<- c(0.1, 0.1, 0.1)			# Point de départ de la recherche d'optimum. De ce point, on converge vers la solution.
P0 		<- c(1, 1, 1)			 	# Valeur initiale des paramètres
Perturbs  <- c(0.2, 0.9, 0.99)	 	# Trois perturbations : -80%, -10%, -1%
MExp_	<- MExp(nbN, Perturbs, P0, F3Nds, st)
MExp_
Res	<- MRARegress(MExp_$Exp, MExp_$Pert, Relative=FALSE)
Res$r
MatrEx <- matrix(c(-1, 3.02204,0, 0,-1, 0.98211, -0.180394,0,-1), nrow=3)
MatrEx
MatDif	<- matrix(0, nrow=2, ncol=nbN*nbN) # 1°ligne, valeurs exactes, 2°ligne, valeurs estimées
for (i in 1:nbN) {
for (j in 1:nbN) {
MatDif[1,(j-1)*nbN+i]	<- MatrEx[i,j]
MatDif[2,(j-1)*nbN+i]	<- Res$r[i,j]
}
}
d1	<- dist(MatDif)
d1
Res$ANOVA
set.seed(12345)
Bruit	<- matrix(rnorm(30, 0, 0.01), nrow=3)
MExp1	<- MExp_$Exp + Bruit
MExp2	<- cbind(MExp_$Exp, MExp1[ ,2:10])
Pert2	<- c(MExp_$Pert, MExp_$Pert[2:10])
Res2	<- MRARegress(MExp2, Pert2, Relative=FALSE)
for (i in 1:nbN) {
for (j in 1:nbN) {
MatDif[1,(j-1)*nbN+i]	<- MatrEx[i,j]
MatDif[2,(j-1)*nbN+i]	<- Res2$r[i,j]
}
}
d2		<- dist(MatDif)
d2
Res2$ANOVA
Res_O2 <- MRARegress(MExp_$Exp,MExp_$Pert, Method="Order2", Relative=FALSE)
Res_O2$r
for (i in 1:nbN) {
for (j in 1:nbN) {
MatDif[1,(j-1)*nbN+i]	<- MatrEx[i,j]
MatDif[2,(j-1)*nbN+i]	<- Res_O2$r[i,j]
}
}
d_O2	<- dist(MatDif)
d_O2
Pas de connaissance à priori :
load("C:\\Users\\jean-pierre.borg\\IRCM\\These\\Recherche\\Packages\\MRAregress\\data\\ Solution_10_1.rda")		#  Solution (diag = 0)
load("C:\\Users\\jean-pierre.borg\\IRCM\\These\\Recherche\\Packages\\MRAregress\\data\\Solution_10_1.rda")		#  Solution (diag = 0)
load("C:\\Users\\jean-pierre.borg\\IRCM\\These\\Recherche\\Packages\\MRAregress\\data\\Perturb2.rda")			#  Perturb2
load("C:\\Users\\jean-pierre.borg\\IRCM\\These\\Recherche\\Packages\\MRAregress\\data\\MatExp_10_1.rda")		#  MatExp_10_1
#' Benchmark
#'
#' @param 	Calc 		Computed matrix (0,1), to compare with reference
#' @param 	Ref			Reference matrix (0,1)
#'
#' @details				Calc and Ref must have the same size and contain 0 or 1 only.
#'
#' @return 	Benchmark	A vector : TP, TN, FP, FN, Sensitivity, Specificity
#'
#'
Benchmark <- function(Calc, Ref) {
if ((dim(Calc)[1] != dim(Ref)[1]) || (dim(Calc)[2] != dim(Ref)[2])) {
print("ERROR : Calc and Ref have different size !")
return(FALSE)
}
diag(Calc)	<- 0														# Diagonal set to 0
diag(Ref)	<- 0
if (!all(Calc %in% c(0:1)) || !all(Ref %in% c(0:1))) {
print("ERROR : Calc and Ref must contain 0 or 1 only !")
return(FALSE)
}
P 	<- sum(Ref)															# True nbr of 1
N 	<- dim(Calc)[1] * (dim(Calc)[2]-1) - P								# True nbr of 0
TP	<- round(sum(ifelse(Calc & Ref, 1,0)), digits=0)					# Nbr of true 1 discovered
TN	<- round(sum(ifelse(!Calc & !Ref, 1,0)) - dim(Calc)[1], digits=0)	# Nbr of true 0 discovered
FP	<- round(N - TN, digits=0)											# Nbr of false 1 discovered
FN	<- round(P - TP	, digits=0)											# Nbr of false 0 discovered
Se 	<- round(TP/P, digits=3)											# Sensibility or sensitivity
Sp	<- round(TN/N, digits=3)											# Specificity
#	cat ("P",P," N",N," TP",TP," TN",TN," FP",FP," FN",FN," Se",Se," Sp",Sp, "\n")
return (c(TP, TN, FP, FN, Se, Sp))
}		# Benchmark
Res_0		<- MRARegress(MatExp_10_1, Perturb2)
MatrCcDig	<- Classify(Res_0)$rDig
Result_0 	<- (Benchmark  (MatrCcDig, Solution))[7]
Result_0
Res_0$r
Solution
MatrCcDig
Benchmark  (MatrCcDig, Solution)
#' Benchmark
#'
#' @param 	Calc 		Computed matrix (0,1), to compare with reference
#' @param 	Ref			Reference matrix (0,1)
#'
#' @details				Calc and Ref must have the same size and contain 0 or 1 only.
#'
#' @return 	Benchmark	A vector : TP, TN, FP, FN, Sensitivity, Specificity, Dist to the diagonal
#'
#'
Benchmark <- function(Calc, Ref) {
if ((dim(Calc)[1] != dim(Ref)[1]) || (dim(Calc)[2] != dim(Ref)[2])) {
print("ERROR : Calc and Ref have different size !")
return(FALSE)
}
diag(Calc)	<- 0														# Diagonal set to 0
diag(Ref)	<- 0
if (!all(Calc %in% c(0, 1)) || !all(Ref %in% c(0, 1))) {
print("ERROR : Calc and Ref must contain 0 or 1 only !")
return(FALSE)
}
P 	<- sum(Ref)															# True nbr of 1
N 	<- dim(Calc)[1] * (dim(Calc)[2]-1) - P								# True nbr of 0
TP	<- round(sum(ifelse(Calc & Ref, 1,0)), digits=0)					# Nbr of true 1 discovered
TN	<- round(sum(ifelse(!Calc & !Ref, 1,0)) - dim(Calc)[1], digits=0)	# Nbr of true 0 discovered
FP	<- round(N - TN, digits=0)											# Nbr of false 1 discovered
FN	<- round(P - TP	, digits=0)											# Nbr of false 0 discovered
Se 	<- round(TP/P, digits=3)											# Sensibility or sensitivity
Sp	<- round(TN/N, digits=3)											# Specificity
Dst	<- Se+Sp-1															# Distance to the diagonal
#	cat ("P",P," N",N," TP",TP," TN",TN," FP",FP," FN",FN," Se",Se," Sp",Sp," Dist ",Dst, "\n")
return (c(TP, TN, FP, FN, Se, Sp, Dst))
}		# Benchmark
Res_0		<- MRARegress(MatExp_10_1, Perturb2)
MatrCcDig	<- Classify(Res_0)$rDig
Result_0 	<- (Benchmark  (MatrCcDig, Solution))[7]
Result_0
nbN	<- 10		# nb. de noeuds
Diag	<- NULL		# Index des termes appartenant à la diagonale
NDiag	<- seq (1, nbN*nbN, by=1)								# Index des termes n'appartenant pas à la diagonale
for (i in 1:nbN) {
Diag   <- cbind (Diag, (i-1)*nbN + i)
}
NDiag	<- NDiag[-Diag]
NDiag
pcent		<- 20
KnlM		<- matrix ("x", nrow=nbN, ncol=nbN)	# Knowledge Map
nbKwn		<- round (pcent*nbN*(nbN-1)/100) # nb. de valeurs connues(20%)
set.seed(12345)
A	<- sample (NDiag, nbKwn)
for (idx in A) {
if (Solution[idx] == 0)
KnlM[idx] 	<- "0"
else if (Solution[idx] > 0)
KnlM[idx] 	<- "1"
else
KnlM[idx] 	<- "-1"
}
Res_20	<- MRARegress(MatExp_10_1, Perturb2, KnlgMap=KnlM, Hyp_Cvx=0.5)
MatrCcDig	<- Classify(Res_20)$rDig
Result_20 	<- (Benchmark  (MatrCcDig, Solution))[7]
Res_20
Result_20
pcent		<- 50
KnlM		<- matrix ("x", nrow=nbN, ncol=nbN)	# Knowledge Map
nbKwn		<- round (pcent*nbN*(nbN-1)/100) # nb. de valeurs connues(20%)
set.seed(12345)
A	<- sample (NDiag, nbKwn)
for (idx in A) {
if (Solution[idx] == 0)
KnlM[idx] 	<- "0"
else if (Solution[idx] > 0)
KnlM[idx] 	<- "1"
else
KnlM[idx] 	<- "-1"
}
Res_50	<- MRARegress(MatExp_10_1, Perturb2, KnlgMap=KnlM, Hyp_Cvx=0.5)
MatrCcDig	<- Classify(Res_50)$rDig
Result_50 	<- (Benchmark  (MatrCcDig, Solution))[7]		# Distance à la diagonale	:
Result_50
vignette("MRARegress", package = "MRARegress")
